{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2359a6e1",
   "metadata": {},
   "source": [
    "# 프로젝트: 한국어 데이터로 챗봇 만들기 C1 류지호"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa0176d",
   "metadata": {},
   "source": [
    "## Step 1. 데이터 수집 및 전처리 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf7a325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발견된 대화파일 수: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "대화파일 파싱: 100%|██████████| 100/100 [00:00<00:00, 1127.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 Q/A 쌍: 3596\n",
      "                                                   Q  \\\n",
      "0              관광 지명에 대한 이해를 높이던 중에 궁금한 점이 생겨서 찾아왔어.   \n",
      "1                            네. 안녕하세요. 어떤 것이 궁금하신가요?   \n",
      "2                                 원주 거돈사지에 대해 알고 싶어!   \n",
      "3  원주 거돈사지(原州 居頓寺址)는 강원특별자치도 원주시 부론면, 현계산 기슭의 작은 ...   \n",
      "4                       오! 절터였구나. 나는 절 특유의 분위기를 좋아해!   \n",
      "\n",
      "                                                   A  \n",
      "0                            네. 안녕하세요. 어떤 것이 궁금하신가요?  \n",
      "1                                 원주 거돈사지에 대해 알고 싶어!  \n",
      "2  원주 거돈사지(原州 居頓寺址)는 강원특별자치도 원주시 부론면, 현계산 기슭의 작은 ...  \n",
      "3                       오! 절터였구나. 나는 절 특유의 분위기를 좋아해!  \n",
      "4                   맞아요. 절에 가면 마음이 잔잔해지고, 마음이 평안해지죠.  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Q_processed  \\\n",
      "0             관광 지명에 대한 이해를 높이던 중에 궁금한 점이 생겨서 찾아왔어 .   \n",
      "1                         네 . 안녕하세요 . 어떤 것이 궁금하신가요 ?   \n",
      "2                                원주 거돈사지에 대해 알고 싶어 !   \n",
      "3  원주 거돈사지 는 강원특별자치도 원주시 부론면 , 현계산 기슭의 작은 골짜기를 끼고...   \n",
      "4                    오 ! 절터였구나 . 나는 절 특유의 분위기를 좋아해 !   \n",
      "\n",
      "                                         A_processed  \n",
      "0                         네 . 안녕하세요 . 어떤 것이 궁금하신가요 ?  \n",
      "1                                원주 거돈사지에 대해 알고 싶어 !  \n",
      "2  원주 거돈사지 는 강원특별자치도 원주시 부론면 , 현계산 기슭의 작은 골짜기를 끼고...  \n",
      "3                    오 ! 절터였구나 . 나는 절 특유의 분위기를 좋아해 !  \n",
      "4                맞아요 . 절에 가면 마음이 잔잔해지고 , 마음이 평안해지죠 .  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "SESSION_FOLDERS = [\n",
    "    \"/aiffel/AIHUB_KoreanMultiSession/2session\",\n",
    "    \"/aiffel/AIHUB_KoreanMultiSession/3session\",\n",
    "    \"/aiffel/AIHUB_KoreanMultiSession/4session\"\n",
    "]\n",
    "\n",
    "json_files = []\n",
    "for folder in SESSION_FOLDERS:\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.json'):\n",
    "                json_files.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"발견된 대화파일 수: {len(json_files)}\")\n",
    "\n",
    "dialog_pairs = []\n",
    "for file in tqdm(json_files, desc=\"대화파일 파싱\"):\n",
    "    with open(file, encoding='utf-8') as f:\n",
    "        session = json.load(f)\n",
    "        # sessionInfo는 리스트(여러 세션)일 수 있음\n",
    "        session_infos = session.get('sessionInfo', [])\n",
    "        if not isinstance(session_infos, list):\n",
    "            session_infos = [session_infos]\n",
    "        for sess in session_infos:\n",
    "            dialog = sess.get('dialog', [])\n",
    "            prev_speaker, prev_text = None, None\n",
    "            for turn in dialog:\n",
    "                speaker = turn.get('speaker')\n",
    "                text = turn.get('utterance')\n",
    "                if text is None or speaker is None:\n",
    "                    continue\n",
    "                text = text.strip()\n",
    "                if prev_speaker is not None and prev_speaker != speaker:\n",
    "                    dialog_pairs.append({'Q': prev_text, 'A': text})\n",
    "                prev_speaker, prev_text = speaker, text\n",
    "\n",
    "print(f\"총 Q/A 쌍: {len(dialog_pairs)}\")\n",
    "\n",
    "df = pd.DataFrame(dialog_pairs)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.to_csv(\"multi_session_qa.csv\", index=False)\n",
    "print(df.head())\n",
    "\n",
    "def preprocess_korean_text(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(r'[ㅋ]{2,}', 'ㅋㅋ', sentence)\n",
    "    sentence = re.sub(r'[ㅎ]{2,}', 'ㅎㅎ', sentence)\n",
    "    sentence = re.sub(r'[ㅜㅠ]{2,}', 'ㅜㅜ', sentence)\n",
    "    sentence = re.sub(r'[ㅡ]{2,}', 'ㅡㅡ', sentence)\n",
    "    sentence = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', sentence)\n",
    "    sentence = re.sub(r'\\S+@\\S+', '[EMAIL]', sentence)\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z0-9?.!,~]+\", \" \", sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    sentence = re.sub(r\"([?.!,~])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    sentence = sentence.strip()\n",
    "    slang_dict = {'갑툭튀': '갑자기 툭 튀어나온', '꾸안꾸': '꾸민 듯 안 꾸민 듯'}\n",
    "    for slang, meaning in slang_dict.items():\n",
    "        sentence = sentence.replace(slang, meaning)\n",
    "    return sentence\n",
    "\n",
    "if not df.empty:\n",
    "    df['Q_processed'] = df['Q'].apply(preprocess_korean_text)\n",
    "    df['A_processed'] = df['A'].apply(preprocess_korean_text)\n",
    "    df.to_csv(\"multi_session_qa_preprocessed.csv\", index=False)\n",
    "    print(df[['Q_processed', 'A_processed']].head())\n",
    "else:\n",
    "    print(\"Q/A 데이터가 없습니다. 폴더/파일/구조를 다시 확인하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f571ba41",
   "metadata": {},
   "source": [
    "## Step 2. 데이터 증강 및 토큰화 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae1361df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화된 질문 shape: (6584, 50)\n",
      "토큰화된 답변 shape: (6584, 50)\n",
      "단어장 크기: 6744\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"multi_session_qa_preprocessed.csv\")\n",
    "questions = df[\"Q_processed\"].astype(str).tolist()\n",
    "answers = df[\"A_processed\"].astype(str).tolist()\n",
    "\n",
    "from konlpy.tag import Mecab, Okt\n",
    "try:\n",
    "    mecab = Mecab()\n",
    "    morpheme_analyzer = mecab\n",
    "except:\n",
    "    morpheme_analyzer = Okt()\n",
    "\n",
    "def enhanced_morpheme_tokenize(sentence):\n",
    "    try:\n",
    "        return ' '.join(morpheme_analyzer.morphs(sentence))\n",
    "    except:\n",
    "        return sentence\n",
    "\n",
    "questions_morpheme = [enhanced_morpheme_tokenize(q) for q in questions]\n",
    "answers_morpheme = [enhanced_morpheme_tokenize(a) for a in answers]\n",
    "\n",
    "augmented_questions = questions_morpheme.copy()\n",
    "augmented_answers = answers_morpheme.copy()\n",
    "for i in range(len(questions_morpheme)):\n",
    "    if len(questions_morpheme[i].split()) > 3:\n",
    "        augmented_questions.append(answers_morpheme[i])\n",
    "        augmented_answers.append(questions_morpheme[i])\n",
    "\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    augmented_questions + augmented_answers, target_vocab_size=2**14)\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "\n",
    "MAX_LENGTH = 50\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "    for q, a in zip(inputs, outputs):\n",
    "        q_tok = START_TOKEN + tokenizer.encode(q) + END_TOKEN\n",
    "        a_tok = START_TOKEN + tokenizer.encode(a) + END_TOKEN\n",
    "        if len(q_tok) <= MAX_LENGTH and len(a_tok) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(q_tok + [0]*(MAX_LENGTH-len(q_tok)))\n",
    "            tokenized_outputs.append(a_tok + [0]*(MAX_LENGTH-len(a_tok)))\n",
    "    return np.array(tokenized_inputs), np.array(tokenized_outputs)\n",
    "\n",
    "questions_tokenized, answers_tokenized = tokenize_and_filter(augmented_questions, augmented_answers)\n",
    "np.save('questions_tokenized.npy', questions_tokenized)\n",
    "np.save('answers_tokenized.npy', answers_tokenized)\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(f\"토큰화된 질문 shape: {questions_tokenized.shape}\")\n",
    "print(f\"토큰화된 답변 shape: {answers_tokenized.shape}\")\n",
    "print(f\"단어장 크기: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56064e9a",
   "metadata": {},
   "source": [
    "## Step 3. 모델 아키텍처 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91ae0d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"improved_transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_inputs (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "context_aware_encoder (Function (None, None, 512)    30316544    inputs[0][0]                     \n",
      "                                                                 enc_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           dec_inputs[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dec_padding_mask (Lambda)       (None, 1, 1, None)   0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "korean_decoder (Functional)     (None, None, 512)    35827712    dec_inputs[0][0]                 \n",
      "                                                                 context_aware_encoder[0][0]      \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "                                                                 dec_padding_mask[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_35 (LayerNo (None, None, 512)    1024        korean_decoder[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, None, 512)    0           layer_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 12000)  6156000     dropout_26[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 72,301,280\n",
      "Trainable params: 72,301,280\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "모델 구조 시각화 실패: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_history'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.position = position\n",
    "        self.d_model = d_model\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "    \n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # 각도 배열 생성\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "        \n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "        \n",
    "        # sin과 cosine이 교차되도록 재배열\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding, [1, 2, 0])\n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "        \n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        config.update({\n",
    "            'position': self.position,\n",
    "            'd_model': self.d_model\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# 스케일드 닷 프로덕트 어텐션 함수 (개선: 어텐션 가중치 반환)\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "    \n",
    "    # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "    \n",
    "    # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "    \n",
    "    # softmax적용\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "    \n",
    "    # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output, attention_weights\n",
    "\n",
    "# 멀티헤드 어텐션 레이어 (개선: 어텐션 가중치 반환 추가)\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "    \n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "        \n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "        \n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "        \n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            query, key, value, mask)\n",
    "        \n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "        \n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        outputs = self.dense(concat_attention)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(MultiHeadAttention, self).get_config()\n",
    "        config.update({\n",
    "            'num_heads': self.num_heads,\n",
    "            'd_model': self.d_model\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# 패딩 마스크 생성 함수\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "# 룩어헤드 마스크 생성 함수\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "# 개선된 인코더 레이어 (드롭아웃 비율 조정 가능)\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    \n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "    \n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "    \n",
    "    # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    attention = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(inputs + attention)\n",
    "    \n",
    "    # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    \n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention + outputs)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "# 개선된 인코더 (임베딩 레이어 분리)\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    \n",
    "    # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "    \n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    \n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "    \n",
    "    # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "# 개선된 디코더 레이어\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': look_ahead_mask\n",
    "        })\n",
    "    \n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "    \n",
    "    # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1,\n",
    "            'key': enc_outputs,\n",
    "            'value': enc_outputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "    \n",
    "    # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention2 + attention1)\n",
    "    \n",
    "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    \n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention2)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "# 개선된 디코더\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"decoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    \n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "    \n",
    "    # num_layers만큼 쌓아올린 디코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "# 맥락 인식 강화 인코더 (메모리 네트워크 추가)\n",
    "def context_aware_encoder(vocab_size,\n",
    "                         num_layers,\n",
    "                         units,\n",
    "                         d_model,\n",
    "                         num_heads,\n",
    "                         dropout,\n",
    "                         memory_size=64,\n",
    "                         name=\"context_aware_encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "    \n",
    "    # 기본 인코더 처리\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "    \n",
    "    # 맥락 메모리 초기화 (배치 크기에 맞게 확장)\n",
    "    batch_size = tf.shape(inputs)[0]\n",
    "    memory = tf.zeros((batch_size, memory_size, d_model))\n",
    "    \n",
    "    # 인코더 레이어 처리 (맥락 메모리 활용)\n",
    "    for i in range(num_layers):\n",
    "        # 기본 인코더 레이어 처리\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "        \n",
    "        # 맥락 메모리 업데이트 (현재 출력과 메모리 결합)\n",
    "        if i < num_layers - 1:  # 마지막 레이어 제외\n",
    "            # 현재 출력의 평균을 메모리에 추가\n",
    "            avg_output = tf.reduce_mean(outputs, axis=1, keepdims=True)\n",
    "            memory = tf.concat([memory[:, 1:, :], avg_output], axis=1)\n",
    "            \n",
    "            # 메모리와 현재 출력 결합\n",
    "            memory_attention = MultiHeadAttention(\n",
    "                d_model, num_heads, name=f\"memory_attention_{i}\")({\n",
    "                    'query': outputs,\n",
    "                    'key': memory,\n",
    "                    'value': memory,\n",
    "                    'mask': None  # 메모리에는 마스크 필요 없음\n",
    "                })\n",
    "            \n",
    "            # 메모리 어텐션 결과와 현재 출력 결합\n",
    "            outputs = tf.keras.layers.LayerNormalization(\n",
    "                epsilon=1e-6)(outputs + memory_attention)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "# 한국어 특화 디코더 (게이트 메커니즘 추가)\n",
    "def korean_decoder(vocab_size,\n",
    "                  num_layers,\n",
    "                  units,\n",
    "                  d_model,\n",
    "                  num_heads,\n",
    "                  dropout,\n",
    "                  name=\"korean_decoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "    \n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    \n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "    \n",
    "    # 이전 레이어의 출력을 저장할 리스트\n",
    "    layer_outputs = [outputs]\n",
    "    \n",
    "    # num_layers만큼 쌓아올린 디코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        # 기본 디코더 레이어 처리\n",
    "        current_outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "        \n",
    "        # 게이트 메커니즘 (이전 레이어 출력과 현재 레이어 출력 결합)\n",
    "        if i > 0:  # 첫 번째 레이어 제외\n",
    "            # 게이트 값 계산\n",
    "            gate = tf.keras.layers.Dense(\n",
    "                d_model, activation='sigmoid', name=f'gate_{i}')(\n",
    "                    tf.concat([outputs, current_outputs], axis=-1))\n",
    "            \n",
    "            # 게이트를 사용하여 이전 출력과 현재 출력 결합\n",
    "            outputs = gate * current_outputs + (1 - gate) * outputs\n",
    "        else:\n",
    "            outputs = current_outputs\n",
    "        \n",
    "        # 현재 레이어 출력 저장\n",
    "        layer_outputs.append(outputs)\n",
    "    \n",
    "    # 모든 레이어의 출력을 결합 (잔차 연결)\n",
    "    all_outputs = tf.concat(layer_outputs, axis=-1)\n",
    "    final_outputs = tf.keras.layers.Dense(d_model, activation='relu')(all_outputs)\n",
    "    \n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=final_outputs,\n",
    "        name=name)\n",
    "\n",
    "# 개선된 트랜스포머 모델 (맥락 인식 및 한국어 특화)\n",
    "def improved_transformer(vocab_size, num_layers, units, d_model, num_heads, dropout):\n",
    "    # 인코더의 입력\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    \n",
    "    # 디코더의 입력\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "    \n",
    "    # 인코더의 패딩 마스크\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "    \n",
    "    # 디코더의 룩어헤드 마스크(첫번째 서브레이어)\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "    \n",
    "    # 디코더의 패딩 마스크(두번째 서브레이어)\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "    \n",
    "    # 인코더 출력\n",
    "    enc_outputs = context_aware_encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "    \n",
    "    # 디코더 출력\n",
    "    dec_outputs = korean_decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "    \n",
    "    # 완전연결층을 통과시켜 단어 예측\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(dec_outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(outputs)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=\"improved_transformer\")\n",
    "\n",
    "# 모델 생성 및 테스트 (예시)\n",
    "if __name__ == \"__main__\":\n",
    "    # 모델 하이퍼파라미터\n",
    "    VOCAB_SIZE = 12000  # 실제 토크나이저에 맞게 조정 필요\n",
    "    NUM_LAYERS = 6\n",
    "    D_MODEL = 512\n",
    "    NUM_HEADS = 8\n",
    "    UNITS = 2048\n",
    "    DROPOUT = 0.1\n",
    "    \n",
    "    # 모델 생성\n",
    "    model = improved_transformer(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        units=UNITS,\n",
    "        d_model=D_MODEL,\n",
    "        num_heads=NUM_HEADS,\n",
    "        dropout=DROPOUT\n",
    "    )\n",
    "    \n",
    "    # 모델 요약 정보 출력\n",
    "    model.summary()\n",
    "    \n",
    "    # 모델 구조 시각화 (옵션)\n",
    "    try:\n",
    "        tf.keras.utils.plot_model(\n",
    "            model,\n",
    "            to_file='improved_transformer_model.png',\n",
    "            show_shapes=True,\n",
    "            show_layer_names=True,\n",
    "            expand_nested=True\n",
    "        )\n",
    "        print(\"모델 구조 이미지가 저장되었습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"모델 구조 시각화 실패: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c4bd9",
   "metadata": {},
   "source": [
    "## Step 4. 모델 학습 및 평가 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e69db910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "93/93 [==============================] - 71s 523ms/step - loss: 3.4752 - accuracy: 0.0349 - val_loss: 3.2455 - val_accuracy: 0.0672\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.24552, saving model to ./checkpoints/transformer/cp-0001.ckpt\n",
      "Epoch 2/30\n",
      "93/93 [==============================] - 46s 491ms/step - loss: 2.6587 - accuracy: 0.0849 - val_loss: 2.7962 - val_accuracy: 0.0998\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.24552 to 2.79619, saving model to ./checkpoints/transformer/cp-0002.ckpt\n",
      "Epoch 3/30\n",
      "93/93 [==============================] - 46s 489ms/step - loss: 2.2899 - accuracy: 0.1105 - val_loss: 2.5223 - val_accuracy: 0.1151\n",
      "\n",
      "Epoch 00003: val_loss improved from 2.79619 to 2.52231, saving model to ./checkpoints/transformer/cp-0003.ckpt\n",
      "Epoch 4/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 2.0529 - accuracy: 0.1276 - val_loss: 2.3350 - val_accuracy: 0.1317\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.52231 to 2.33502, saving model to ./checkpoints/transformer/cp-0004.ckpt\n",
      "Epoch 5/30\n",
      "93/93 [==============================] - 46s 491ms/step - loss: 1.8777 - accuracy: 0.1439 - val_loss: 2.1992 - val_accuracy: 0.1439\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.33502 to 2.19921, saving model to ./checkpoints/transformer/cp-0005.ckpt\n",
      "Epoch 6/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 1.7331 - accuracy: 0.1574 - val_loss: 2.0762 - val_accuracy: 0.1564\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.19921 to 2.07625, saving model to ./checkpoints/transformer/cp-0006.ckpt\n",
      "Epoch 7/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 1.6083 - accuracy: 0.1698 - val_loss: 1.9832 - val_accuracy: 0.1654\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.07625 to 1.98322, saving model to ./checkpoints/transformer/cp-0007.ckpt\n",
      "Epoch 8/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 1.4957 - accuracy: 0.1829 - val_loss: 1.8837 - val_accuracy: 0.1764\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.98322 to 1.88370, saving model to ./checkpoints/transformer/cp-0008.ckpt\n",
      "Epoch 9/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 1.3883 - accuracy: 0.1964 - val_loss: 1.7988 - val_accuracy: 0.1856\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.88370 to 1.79884, saving model to ./checkpoints/transformer/cp-0009.ckpt\n",
      "Epoch 10/30\n",
      "93/93 [==============================] - 46s 491ms/step - loss: 1.2902 - accuracy: 0.2086 - val_loss: 1.7074 - val_accuracy: 0.1963\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.79884 to 1.70738, saving model to ./checkpoints/transformer/cp-0010.ckpt\n",
      "Epoch 11/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 1.1934 - accuracy: 0.2216 - val_loss: 1.6322 - val_accuracy: 0.2040\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.70738 to 1.63221, saving model to ./checkpoints/transformer/cp-0011.ckpt\n",
      "Epoch 12/30\n",
      "93/93 [==============================] - 46s 491ms/step - loss: 1.1037 - accuracy: 0.2342 - val_loss: 1.5514 - val_accuracy: 0.2148\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.63221 to 1.55135, saving model to ./checkpoints/transformer/cp-0012.ckpt\n",
      "Epoch 13/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 1.0175 - accuracy: 0.2467 - val_loss: 1.4727 - val_accuracy: 0.2249\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.55135 to 1.47272, saving model to ./checkpoints/transformer/cp-0013.ckpt\n",
      "Epoch 14/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.9345 - accuracy: 0.2594 - val_loss: 1.4037 - val_accuracy: 0.2350\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.47272 to 1.40366, saving model to ./checkpoints/transformer/cp-0014.ckpt\n",
      "Epoch 15/30\n",
      "93/93 [==============================] - 46s 491ms/step - loss: 0.8568 - accuracy: 0.2722 - val_loss: 1.3405 - val_accuracy: 0.2433\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.40366 to 1.34046, saving model to ./checkpoints/transformer/cp-0015.ckpt\n",
      "Epoch 16/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.7830 - accuracy: 0.2844 - val_loss: 1.2658 - val_accuracy: 0.2531\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.34046 to 1.26581, saving model to ./checkpoints/transformer/cp-0016.ckpt\n",
      "Epoch 17/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.7145 - accuracy: 0.2958 - val_loss: 1.2025 - val_accuracy: 0.2639\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.26581 to 1.20248, saving model to ./checkpoints/transformer/cp-0017.ckpt\n",
      "Epoch 18/30\n",
      "93/93 [==============================] - 46s 491ms/step - loss: 0.6498 - accuracy: 0.3077 - val_loss: 1.1504 - val_accuracy: 0.2730\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.20248 to 1.15043, saving model to ./checkpoints/transformer/cp-0018.ckpt\n",
      "Epoch 19/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.5895 - accuracy: 0.3185 - val_loss: 1.0961 - val_accuracy: 0.2809\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.15043 to 1.09608, saving model to ./checkpoints/transformer/cp-0019.ckpt\n",
      "Epoch 20/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.5338 - accuracy: 0.3289 - val_loss: 1.0487 - val_accuracy: 0.2885\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.09608 to 1.04866, saving model to ./checkpoints/transformer/cp-0020.ckpt\n",
      "Epoch 21/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.4813 - accuracy: 0.3385 - val_loss: 0.9989 - val_accuracy: 0.2982\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.04866 to 0.99893, saving model to ./checkpoints/transformer/cp-0021.ckpt\n",
      "Epoch 22/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.4347 - accuracy: 0.3464 - val_loss: 0.9502 - val_accuracy: 0.3079\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.99893 to 0.95018, saving model to ./checkpoints/transformer/cp-0022.ckpt\n",
      "Epoch 23/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.3900 - accuracy: 0.3535 - val_loss: 0.9212 - val_accuracy: 0.3130\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.95018 to 0.92118, saving model to ./checkpoints/transformer/cp-0023.ckpt\n",
      "Epoch 24/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.3541 - accuracy: 0.3585 - val_loss: 0.8793 - val_accuracy: 0.3218\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.92118 to 0.87925, saving model to ./checkpoints/transformer/cp-0024.ckpt\n",
      "Epoch 25/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.3184 - accuracy: 0.3634 - val_loss: 0.8489 - val_accuracy: 0.3319\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.87925 to 0.84889, saving model to ./checkpoints/transformer/cp-0025.ckpt\n",
      "Epoch 26/30\n",
      "93/93 [==============================] - 46s 491ms/step - loss: 0.2880 - accuracy: 0.3673 - val_loss: 0.8271 - val_accuracy: 0.3344\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.84889 to 0.82709, saving model to ./checkpoints/transformer/cp-0026.ckpt\n",
      "Epoch 27/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.2601 - accuracy: 0.3711 - val_loss: 0.7956 - val_accuracy: 0.3442\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.82709 to 0.79565, saving model to ./checkpoints/transformer/cp-0027.ckpt\n",
      "Epoch 28/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.2353 - accuracy: 0.3738 - val_loss: 0.7788 - val_accuracy: 0.3465\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.79565 to 0.77876, saving model to ./checkpoints/transformer/cp-0028.ckpt\n",
      "Epoch 29/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.2138 - accuracy: 0.3765 - val_loss: 0.7642 - val_accuracy: 0.3503\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.77876 to 0.76420, saving model to ./checkpoints/transformer/cp-0029.ckpt\n",
      "Epoch 30/30\n",
      "93/93 [==============================] - 46s 490ms/step - loss: 0.1953 - accuracy: 0.3783 - val_loss: 0.7537 - val_accuracy: 0.3523\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.76420 to 0.75371, saving model to ./checkpoints/transformer/cp-0030.ckpt\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 ./checkpoints/transformer/final_model에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "train_size = int(0.9 * len(questions_tokenized))\n",
    "train_questions = questions_tokenized[:train_size]\n",
    "train_answers = answers_tokenized[:train_size]\n",
    "val_questions = questions_tokenized[train_size:]\n",
    "val_answers = answers_tokenized[train_size:]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'inputs': train_questions, 'dec_inputs': train_answers[:, :-1]},\n",
    "    {'outputs': train_answers[:, 1:]}\n",
    ")).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'inputs': val_questions, 'dec_inputs': val_answers[:, :-1]},\n",
    "    {'outputs': val_answers[:, 1:]}\n",
    ")).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "def weighted_loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')(y_true, y_pred)\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    weighted_mask = tf.where(mask > 0, 1.2 * mask, mask)\n",
    "    loss = tf.multiply(loss, weighted_mask)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4, beta_1=0.9, beta_2=0.98, epsilon=1e-9),\n",
    "    loss=weighted_loss_function,\n",
    "    metrics=[accuracy]\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint_dir = \"./checkpoints/transformer\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"cp-{epoch:04d}.ckpt\")\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, save_weights_only=True, verbose=1, save_best_only=True, monitor='val_loss')\n",
    "\n",
    "EPOCHS = 30\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, cp_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='validation')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "final_model_path = os.path.join(checkpoint_dir, \"final_model\")\n",
    "model.save_weights(final_model_path)\n",
    "print(f\"모델이 {final_model_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec1c1e0",
   "metadata": {},
   "source": [
    "## Step 5. 추론 및 대화 인터페이스 개선"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c767a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저 모델과 토크나이저를 로드합니다\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 토크나이저 로드\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# 모델 하이퍼파라미터 설정 (훈련에 사용된 것과 동일해야 함)\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2  # start, end 토큰 포함\n",
    "NUM_LAYERS = 6\n",
    "D_MODEL = 512\n",
    "NUM_HEADS = 8\n",
    "UNITS = 2048\n",
    "DROPOUT = 0.1\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "# 모델 재생성\n",
    "from improved_transformer import improved_transformer  # 모델 정의가 있는 파일을 import\n",
    "\n",
    "model = improved_transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# 저장된 가중치 로드\n",
    "model.load_weights(\"./checkpoints/transformer/final_model\")\n",
    "\n",
    "\n",
    "class KoreanChatbotSession:\n",
    "    def __init__(self, model, tokenizer, max_length=50, context_turns=3):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.start_token = tokenizer.vocab_size\n",
    "        self.end_token = tokenizer.vocab_size + 1\n",
    "        self.context = []\n",
    "        self.context_turns = context_turns\n",
    "    \n",
    "    def reset_context(self):\n",
    "        self.context = []\n",
    "    \n",
    "    def build_context_input(self, user_input):\n",
    "        context_text = \"\"\n",
    "        for turn in self.context[-self.context_turns*2:]:\n",
    "            prefix = \"사용자: \" if turn['user']=='user' else \"챗봇: \"\n",
    "            context_text += f\"{prefix}{turn['text']}\\n\"\n",
    "        context_text += f\"사용자: {user_input}\\n챗봇: \"\n",
    "        return context_text.strip()\n",
    "    \n",
    "    def preprocess_input(self, text):\n",
    "        tokens = [self.start_token] + self.tokenizer.encode(text) + [self.end_token]\n",
    "        tokens = tokens[:self.max_length] + [0]*(self.max_length-len(tokens[:self.max_length]))\n",
    "        return tf.expand_dims(tokens, 0)\n",
    "    \n",
    "    def generate_response(self, user_input, temperature=0.7, max_length=50):\n",
    "        try:\n",
    "            context_input = self.build_context_input(user_input)\n",
    "            encoder_input = self.preprocess_input(context_input)\n",
    "            decoder_input = tf.expand_dims([self.start_token], 0)\n",
    "            \n",
    "            # 수정된 부분: * 대신 i 사용\n",
    "            for i in range(max_length):\n",
    "                preds = self.model([encoder_input, decoder_input], training=False)\n",
    "                preds = preds[:, -1, :] / temperature\n",
    "                pred_id = tf.random.categorical(preds, 1)[0,0].numpy()\n",
    "                \n",
    "                if pred_id == self.end_token:\n",
    "                    break\n",
    "                \n",
    "                decoder_input = tf.concat([decoder_input, tf.expand_dims([pred_id], 0)], axis=-1)\n",
    "            \n",
    "            output_tokens = [int(i) for i in decoder_input[0].numpy()]\n",
    "            if output_tokens[0] == self.start_token:\n",
    "                output_tokens = output_tokens[1:]\n",
    "            if self.end_token in output_tokens:\n",
    "                output_tokens = output_tokens[:output_tokens.index(self.end_token)]\n",
    "            \n",
    "            output_text = self.tokenizer.decode(output_tokens)\n",
    "            self.context.append({'user':'user','text':user_input})\n",
    "            self.context.append({'user':'bot','text':output_text})\n",
    "            return output_text\n",
    "        except Exception as e:\n",
    "            print(f\"응답 생성 중 오류 발생: {e}\")\n",
    "            return \"죄송합니다, 응답을 생성하는 데 문제가 발생했습니다.\"\n",
    "\n",
    "\n",
    "def run_chatbot_cli(model, tokenizer):\n",
    "    print(\"모델을 초기화하는 중...\")\n",
    "    session = KoreanChatbotSession(model, tokenizer)\n",
    "    print(\"ChatGBT입니다. '종료' 입력 시 종료.\")\n",
    "    while True:\n",
    "        user_input = input(\"사용자: \").strip()\n",
    "        if user_input.lower() in ['종료', 'quit', 'exit']:\n",
    "            print(\"챗봇 세션 종료.\")\n",
    "            break\n",
    "        elif user_input.lower() in ['새로시작', 'reset', 'restart']:\n",
    "            session.reset_context()\n",
    "            print(\"대화 맥락이 초기화되었습니다.\")\n",
    "            continue\n",
    "        elif not user_input:\n",
    "            continue\n",
    "        print(\"챗봇:\", session.generate_response(user_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_chatbot_cli(model, tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
